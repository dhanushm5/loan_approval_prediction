{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":76849,"databundleVersionId":8347608,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader\n!pip install optuna\nimport optuna\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load Data\ntrain_df = pd.read_csv('/kaggle/input/round-2-nexus-recruitment/train.csv')\ntest_df = pd.read_csv('/kaggle/input/round-2-nexus-recruitment/test.csv')\n\ntrain_df.head()\ntrain_df.info()\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-03T16:34:23.511156Z","iopub.execute_input":"2024-11-03T16:34:23.511699Z","iopub.status.idle":"2024-11-03T16:34:35.039296Z","shell.execute_reply.started":"2024-11-03T16:34:23.511658Z","shell.execute_reply":"2024-11-03T16:34:35.038153Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (4.0.0)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.13.3)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna) (6.8.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (21.3)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (2.0.30)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna) (4.66.4)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0.2)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->optuna) (3.1.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 15000 entries, 0 to 14999\nData columns (total 13 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   id                     15000 non-null  int64  \n 1   Hardness               15000 non-null  float64\n 2   allelectrons_Total     15000 non-null  float64\n 3   density_Total          15000 non-null  float64\n 4   allelectrons_Average   15000 non-null  float64\n 5   val_e_Average          15000 non-null  float64\n 6   atomicweight_Average   15000 non-null  float64\n 7   ionenergy_Average      15000 non-null  float64\n 8   el_neg_chi_Average     15000 non-null  float64\n 9   R_vdw_element_Average  15000 non-null  float64\n 10  R_cov_element_Average  15000 non-null  float64\n 11  zaratio_Average        15000 non-null  float64\n 12  density_Average        15000 non-null  float64\ndtypes: float64(12), int64(1)\nmemory usage: 1.5 MB\n/kaggle/input/round-2-nexus-recruitment/sample_submission.csv\n/kaggle/input/round-2-nexus-recruitment/train.csv\n/kaggle/input/round-2-nexus-recruitment/test.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Feature Engineering\ndef create_features(df):\n    df['density_ratio'] = df['density_Total'] / df['density_Average']\n    df['electron_density_ratio'] = df['allelectrons_Total'] / df['density_Total']\n    df['weight_density_ratio'] = df['atomicweight_Average'] / df['density_Average']\n    df['ionenergy_density'] = df['ionenergy_Average'] / df['density_Average']\n    df['log_density'] = np.log1p(df['density_Total'])\n    df['log_weight'] = np.log1p(df['atomicweight_Average'])\n    df['energy_density_product'] = df['ionenergy_Average'] * df['density_Total']\n    df['electron_weight_ratio'] = df['allelectrons_Total'] / df['atomicweight_Average']\n    df['total_energy'] = df['allelectrons_Total'] * df['ionenergy_Average']\n    return df\n\ntrain_df = create_features(train_df)\ntest_df = create_features(test_df)\n\n# Handle infinite values\ntrain_df.replace([np.inf, -np.inf], np.nan, inplace=True)\ntest_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# Fill missing values\ntrain_df.fillna(train_df.mean(), inplace=True)\ntest_df.fillna(test_df.mean(), inplace=True)\n\n# Prepare Data\nX = train_df.drop(['Hardness', 'id'], axis=1)\ny = train_df['Hardness']\nX_test = test_df.drop(['id'], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T16:35:05.902087Z","iopub.execute_input":"2024-11-03T16:35:05.902729Z","iopub.status.idle":"2024-11-03T16:35:05.952834Z","shell.execute_reply.started":"2024-11-03T16:35:05.902686Z","shell.execute_reply":"2024-11-03T16:35:05.951812Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Polynomial Features\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\nX_poly = poly.fit_transform(X)\nX_test_poly = poly.transform(X_test)\n\n# Scaling\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_poly)\nX_test_scaled = scaler.transform(X_test_poly)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T16:35:51.393444Z","iopub.execute_input":"2024-11-03T16:35:51.394305Z","iopub.status.idle":"2024-11-03T16:35:51.508603Z","shell.execute_reply.started":"2024-11-03T16:35:51.394263Z","shell.execute_reply":"2024-11-03T16:35:51.507519Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Split Data\nX_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n# Convert to Tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\ny_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1).to(device)\nX_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T16:37:10.897094Z","iopub.execute_input":"2024-11-03T16:37:10.897912Z","iopub.status.idle":"2024-11-03T16:37:11.105088Z","shell.execute_reply.started":"2024-11-03T16:37:10.897869Z","shell.execute_reply":"2024-11-03T16:37:11.104146Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Objective Function for Optuna\ndef objective(trial):\n    # Hyperparameters\n    hidden_size = trial.suggest_int('hidden_size', 128, 1024)\n    num_layers = trial.suggest_int('num_layers', 1, 5)\n    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n    activation_name = trial.suggest_categorical('activation', ['ReLU', 'LeakyReLU', 'ELU', 'GELU'])\n    \n    # Activation Function\n    activation = getattr(nn, activation_name)()\n    \n    # Model Definition\n    class NeuralNet(nn.Module):\n        def __init__(self, input_dim):\n            super(NeuralNet, self).__init__()\n            layers = []\n            in_features = input_dim\n            for _ in range(num_layers):\n                layers.append(nn.Linear(in_features, hidden_size))\n                layers.append(activation)\n                layers.append(nn.Dropout(dropout_rate))\n                in_features = hidden_size\n            layers.append(nn.Linear(hidden_size, 1))\n            self.model = nn.Sequential(*layers)\n    \n        def forward(self, x):\n            return self.model(x)\n    \n    # Model and Optimizer\n    model = NeuralNet(input_dim=X_train.shape[1])\n    model.to(device)\n    \n    # Loss and Optimizer\n    criterion = nn.L1Loss()\n    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    \n    # Data Loaders\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    # Training Loop\n    num_epochs = 50\n    for epoch in range(num_epochs):\n        model.train()\n        for X_batch, y_batch in train_loader:\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for X_batch, y_batch in val_loader:\n                X_batch = X_batch.to(device)\n                y_batch = y_batch.to(device)\n                outputs = model(X_batch)\n                loss = criterion(outputs, y_batch)\n                val_loss += loss.item() * X_batch.size(0)\n        val_loss /= len(val_dataset)\n        \n        # Report\n        trial.report(val_loss, epoch)\n        \n        # Prune\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n    \n    return val_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T16:37:30.959240Z","iopub.execute_input":"2024-11-03T16:37:30.960153Z","iopub.status.idle":"2024-11-03T16:37:30.976252Z","shell.execute_reply.started":"2024-11-03T16:37:30.960110Z","shell.execute_reply":"2024-11-03T16:37:30.975246Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Run Optuna Study\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\n\nprint('Best trial:')\ntrial = study.best_trial\n\nprint('  Validation Loss: {:.5f}'.format(trial.value))\nprint('  Best hyperparameters: {}'.format(trial.params))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T16:37:47.068178Z","iopub.execute_input":"2024-11-03T16:37:47.068595Z","iopub.status.idle":"2024-11-03T16:51:09.557856Z","shell.execute_reply.started":"2024-11-03T16:37:47.068550Z","shell.execute_reply":"2024-11-03T16:51:09.556838Z"}},"outputs":[{"name":"stderr","text":"[I 2024-11-03 16:37:47,070] A new study created in memory with name: no-name-8fa14b46-4442-4832-8d9c-b5a1a89a8f52\n/tmp/ipykernel_30/961124132.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n/tmp/ipykernel_30/961124132.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n[I 2024-11-03 16:38:37,836] Trial 0 finished with value: 1.1363405629793804 and parameters: {'hidden_size': 434, 'num_layers': 4, 'dropout_rate': 0.36990611339712093, 'learning_rate': 0.00369871708154812, 'weight_decay': 0.0006736539325875061, 'batch_size': 32, 'activation': 'ReLU'}. Best is trial 0 with value: 1.1363405629793804.\n/tmp/ipykernel_30/961124132.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n/tmp/ipykernel_30/961124132.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n[I 2024-11-03 16:39:07,296] Trial 1 finished with value: 1.007044596195221 and parameters: {'hidden_size': 589, 'num_layers': 5, 'dropout_rate': 0.19537501504869892, 'learning_rate': 0.0001689903019406748, 'weight_decay': 7.298850614598229e-05, 'batch_size': 64, 'activation': 'ELU'}. Best is trial 1 with value: 1.007044596195221.\n[I 2024-11-03 16:39:21,699] Trial 2 finished with value: 1.0118445591926575 and parameters: {'hidden_size': 501, 'num_layers': 2, 'dropout_rate': 0.34049512214216815, 'learning_rate': 8.642889371570567e-05, 'weight_decay': 0.0020272967722340133, 'batch_size': 128, 'activation': 'ReLU'}. Best is trial 1 with value: 1.007044596195221.\n[I 2024-11-03 16:39:41,513] Trial 3 finished with value: 1.083197106997172 and parameters: {'hidden_size': 740, 'num_layers': 1, 'dropout_rate': 0.4284684764792459, 'learning_rate': 0.00012430558540949945, 'weight_decay': 3.9973898975086936e-05, 'batch_size': 64, 'activation': 'ELU'}. Best is trial 1 with value: 1.007044596195221.\n[I 2024-11-03 16:39:54,789] Trial 4 finished with value: 1.0315184520085652 and parameters: {'hidden_size': 805, 'num_layers': 1, 'dropout_rate': 0.1266622982559371, 'learning_rate': 0.0018806192898026257, 'weight_decay': 1.5350824773021778e-05, 'batch_size': 128, 'activation': 'ELU'}. Best is trial 1 with value: 1.007044596195221.\n[I 2024-11-03 16:39:55,060] Trial 5 pruned. \n[I 2024-11-03 16:40:17,435] Trial 6 finished with value: 1.0094916524887085 and parameters: {'hidden_size': 227, 'num_layers': 2, 'dropout_rate': 0.368669389073952, 'learning_rate': 0.0015156683888840855, 'weight_decay': 0.0010139598913152259, 'batch_size': 64, 'activation': 'ReLU'}. Best is trial 1 with value: 1.007044596195221.\n[I 2024-11-03 16:40:44,658] Trial 7 finished with value: 1.0119101430575053 and parameters: {'hidden_size': 474, 'num_layers': 4, 'dropout_rate': 0.13992511436833, 'learning_rate': 8.526196193567656e-05, 'weight_decay': 0.0014387299559743714, 'batch_size': 64, 'activation': 'ELU'}. Best is trial 1 with value: 1.007044596195221.\n[I 2024-11-03 16:40:45,064] Trial 8 pruned. \n[I 2024-11-03 16:41:17,222] Trial 9 pruned. \n[I 2024-11-03 16:41:17,871] Trial 10 pruned. \n[I 2024-11-03 16:41:42,357] Trial 11 finished with value: 0.9978911825815837 and parameters: {'hidden_size': 251, 'num_layers': 3, 'dropout_rate': 0.46916567453305913, 'learning_rate': 0.0007618901300822642, 'weight_decay': 0.00018759275496992744, 'batch_size': 64, 'activation': 'GELU'}. Best is trial 11 with value: 0.9978911825815837.\n[I 2024-11-03 16:42:09,330] Trial 12 finished with value: 0.9931070696512858 and parameters: {'hidden_size': 330, 'num_layers': 4, 'dropout_rate': 0.47423226762079324, 'learning_rate': 0.0004785916699644527, 'weight_decay': 0.0001582601012640085, 'batch_size': 64, 'activation': 'GELU'}. Best is trial 12 with value: 0.9931070696512858.\n[I 2024-11-03 16:42:33,950] Trial 13 finished with value: 0.9876830488840739 and parameters: {'hidden_size': 346, 'num_layers': 3, 'dropout_rate': 0.49171178159480583, 'learning_rate': 0.0005041727750896875, 'weight_decay': 0.00025346548414033343, 'batch_size': 64, 'activation': 'GELU'}. Best is trial 13 with value: 0.9876830488840739.\n[I 2024-11-03 16:42:34,530] Trial 14 pruned. \n[I 2024-11-03 16:43:17,530] Trial 15 finished with value: 0.9844130560557047 and parameters: {'hidden_size': 330, 'num_layers': 3, 'dropout_rate': 0.43288029489572893, 'learning_rate': 0.0004848711670910052, 'weight_decay': 0.00032179388025167544, 'batch_size': 32, 'activation': 'GELU'}. Best is trial 15 with value: 0.9844130560557047.\n[I 2024-11-03 16:44:00,208] Trial 16 finished with value: 0.9915698370933532 and parameters: {'hidden_size': 142, 'num_layers': 3, 'dropout_rate': 0.4143351930989571, 'learning_rate': 0.0003689166720188368, 'weight_decay': 0.0004247927051320792, 'batch_size': 32, 'activation': 'GELU'}. Best is trial 15 with value: 0.9844130560557047.\n[I 2024-11-03 16:44:01,085] Trial 17 pruned. \n[I 2024-11-03 16:44:02,642] Trial 18 pruned. \n[I 2024-11-03 16:44:45,232] Trial 19 finished with value: 0.9929739667574564 and parameters: {'hidden_size': 563, 'num_layers': 3, 'dropout_rate': 0.3194331318302918, 'learning_rate': 0.0008946280434635253, 'weight_decay': 0.0003346614949374435, 'batch_size': 32, 'activation': 'GELU'}. Best is trial 15 with value: 0.9844130560557047.\n[I 2024-11-03 16:44:46,202] Trial 20 pruned. \n[I 2024-11-03 16:44:47,936] Trial 21 pruned. \n[I 2024-11-03 16:45:30,676] Trial 22 finished with value: 0.9924076690673828 and parameters: {'hidden_size': 207, 'num_layers': 3, 'dropout_rate': 0.4986613415814705, 'learning_rate': 0.0004732330557712279, 'weight_decay': 0.00035016522208525746, 'batch_size': 32, 'activation': 'GELU'}. Best is trial 15 with value: 0.9844130560557047.\n[I 2024-11-03 16:46:13,356] Trial 23 finished with value: 0.9858661416371663 and parameters: {'hidden_size': 296, 'num_layers': 3, 'dropout_rate': 0.4077103587273512, 'learning_rate': 0.0004812771317207728, 'weight_decay': 5.758129296519203e-05, 'batch_size': 32, 'activation': 'GELU'}. Best is trial 15 with value: 0.9844130560557047.\n[I 2024-11-03 16:46:14,146] Trial 24 pruned. \n[I 2024-11-03 16:46:14,496] Trial 25 pruned. \n[I 2024-11-03 16:47:01,520] Trial 26 finished with value: 0.9814635081291199 and parameters: {'hidden_size': 404, 'num_layers': 4, 'dropout_rate': 0.3896605996656132, 'learning_rate': 0.0005870198282136842, 'weight_decay': 9.845175797028525e-05, 'batch_size': 32, 'activation': 'GELU'}. Best is trial 26 with value: 0.9814635081291199.\n[I 2024-11-03 16:47:02,611] Trial 27 pruned. \n[I 2024-11-03 16:47:49,952] Trial 28 finished with value: 0.9915777015686035 and parameters: {'hidden_size': 279, 'num_layers': 4, 'dropout_rate': 0.22779548470463296, 'learning_rate': 0.0006982285275598284, 'weight_decay': 3.1039389964301306e-06, 'batch_size': 32, 'activation': 'GELU'}. Best is trial 26 with value: 0.9814635081291199.\n[I 2024-11-03 16:47:50,962] Trial 29 pruned. \n[I 2024-11-03 16:47:57,671] Trial 30 pruned. \n[I 2024-11-03 16:48:40,418] Trial 31 finished with value: 0.9891109053293864 and parameters: {'hidden_size': 305, 'num_layers': 3, 'dropout_rate': 0.45032673652247207, 'learning_rate': 0.000539138657054033, 'weight_decay': 0.00021205324684638185, 'batch_size': 32, 'activation': 'GELU'}. Best is trial 26 with value: 0.9814635081291199.\n[I 2024-11-03 16:48:41,319] Trial 32 pruned. \n[I 2024-11-03 16:48:41,648] Trial 33 pruned. \n[I 2024-11-03 16:48:42,716] Trial 34 pruned. \n[I 2024-11-03 16:48:43,256] Trial 35 pruned. \n[I 2024-11-03 16:48:43,628] Trial 36 pruned. \n[I 2024-11-03 16:48:44,411] Trial 37 pruned. \n[I 2024-11-03 16:48:44,938] Trial 38 pruned. \n[I 2024-11-03 16:48:45,327] Trial 39 pruned. \n[I 2024-11-03 16:48:46,136] Trial 40 pruned. \n[I 2024-11-03 16:48:47,062] Trial 41 pruned. \n[I 2024-11-03 16:49:29,705] Trial 42 finished with value: 0.9845227530797322 and parameters: {'hidden_size': 305, 'num_layers': 3, 'dropout_rate': 0.4542595392180756, 'learning_rate': 0.00039018328000623765, 'weight_decay': 0.0001204904759957118, 'batch_size': 32, 'activation': 'GELU'}. Best is trial 26 with value: 0.9814635081291199.\n[I 2024-11-03 16:50:12,457] Trial 43 finished with value: 0.9890268545150757 and parameters: {'hidden_size': 372, 'num_layers': 3, 'dropout_rate': 0.46860372061613054, 'learning_rate': 0.00031829473264595925, 'weight_decay': 0.00010058803464822505, 'batch_size': 32, 'activation': 'GELU'}. Best is trial 26 with value: 0.9814635081291199.\n[I 2024-11-03 16:50:22,544] Trial 44 pruned. \n[I 2024-11-03 16:51:05,702] Trial 45 finished with value: 0.9811876748402913 and parameters: {'hidden_size': 477, 'num_layers': 3, 'dropout_rate': 0.35566499991849154, 'learning_rate': 0.00041834363876298495, 'weight_decay': 2.8156448414723406e-05, 'batch_size': 32, 'activation': 'GELU'}. Best is trial 45 with value: 0.9811876748402913.\n[I 2024-11-03 16:51:06,739] Trial 46 pruned. \n[I 2024-11-03 16:51:07,554] Trial 47 pruned. \n[I 2024-11-03 16:51:08,636] Trial 48 pruned. \n[I 2024-11-03 16:51:09,552] Trial 49 pruned. \n","output_type":"stream"},{"name":"stdout","text":"Best trial:\n  Validation Loss: 0.98119\n  Best hyperparameters: {'hidden_size': 477, 'num_layers': 3, 'dropout_rate': 0.35566499991849154, 'learning_rate': 0.00041834363876298495, 'weight_decay': 2.8156448414723406e-05, 'batch_size': 32, 'activation': 'GELU'}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Train Best Model\nbest_params = trial.params\nactivation = getattr(nn, best_params['activation'])()\n\nclass NeuralNet(nn.Module):\n    def __init__(self, input_dim):\n        super(NeuralNet, self).__init__()\n        layers = []\n        in_features = input_dim\n        for _ in range(best_params['num_layers']):\n            layers.append(nn.Linear(in_features, best_params['hidden_size']))\n            layers.append(activation)\n            layers.append(nn.Dropout(best_params['dropout_rate']))\n            in_features = best_params['hidden_size']\n        layers.append(nn.Linear(best_params['hidden_size'], 1))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\nmodel = NeuralNet(input_dim=X_train.shape[1])\nmodel.to(device)\n\ncriterion = nn.L1Loss()\noptimizer = optim.AdamW(model.parameters(),\n                        lr=best_params['learning_rate'],\n                        weight_decay=best_params['weight_decay'])\nbatch_size = best_params['batch_size']\n\ntrain_dataset = TensorDataset(X_tensor, y_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    model.train()\n    for X_batch, y_batch in train_loader:\n        X_batch = X_batch.to(device)\n        y_batch = y_batch.to(device)\n        optimizer.zero_grad()\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        optimizer.step()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prediction\nmodel.eval()\nwith torch.no_grad():\n    X_test_device = X_test_tensor.to(device)\n    predictions = model(X_test_device)\n    final_preds = predictions.cpu().numpy().squeeze()\n\n# Submission\nsubmission = pd.DataFrame({\n    'id': test_df['id'],\n    'Hardness': final_preds\n})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}